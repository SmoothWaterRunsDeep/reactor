这部分代码实现了一个初步的reactor服务器模型的功能，但是还没有写客户端部分的代码。这个可以用Linux的nc命令来测试TCP线路。开启服务器后，使用nc ip port命令即可
另外V1也没有实现IO多路复用，所以在V2中我们应该把epoll再封装进去。

v2:外部代码（例如服务器的业务逻辑部分）并不直接与 TcpConnection 交互，而是先将处理这三个事件的回调函数注册给 EventLoop。这是因为 EventLoop 负责整个事件循环，它掌控着新连接的接受、已有连接上数据的读写以及连接断开等操作的时机。
EventLoop 先将这些回调函数保存起来，当 EventLoop 检测到有新的连接请求到来时，会创建一个 TcpConnection 对象来代表这个新的连接。然后，EventLoop 会把之前保存的三个回调函数注册到这个新创建的 TcpConnection 对象中。
这里，EventLoop 起到了一个中介的作用，它将外部的回调函数传递给具体的 TcpConnection 对象，使得每个 TcpConnection 对象在相应事件发生时能够执行正确的处理逻辑。

bug1：如果在连接过程中关于shared_from_this()报错了，可能是因为没有公有继承enabled_shared_from_this<TcpConnection>,因为类中默认私有继承。由于类中的函数的声明和定义是分开写的，在类外执行shared_from_this()函数将会报错。
bug2：如果遇到服务器不能正常显示收到的数据可能是因为SockIo.cc中的read函数阻塞住了。注意在readLine函数中的recv函数的第四个参数一定要设置为MSG_PEEK。


v3:这个版本是对v2版本的进一步封装，其实并没有很多变动，因为在v2版本的testEventLoop函数中的test函数中的逻辑还是有点多了，所以我们对test中的逻辑进一步封装
把创建acceptor和Eventloop对象的这些行为给封装在一个新的类中，称它为TcpServer。因为test函数干的不就是一个服务器干的事吗，创建监听状态的描述符并放在epoll
实例中进行监听，把三个半事件进行注册，把loop给运行起来。这样封装之后，暴露出的接口就能进一步简化。


v4:在大量任务的情况下，仅让reactor进行串行处理是不能满足实时性的需求的，我们需要引入线程池。但是注意，reactor把数据发送给线程池让它处理后不应该让线程池
承担数据的发送，因为cpu是高速设备，它去做运算类的任务更能发挥性能，如果你在线程里实现数据的发送那岂不是每一个线程都要去和网卡进行交互。所以线程池处理好数据之后
应该将数据反馈给reactor，让它进行数据的发送。一个线程去和网卡交互总比所有线程都和网卡交互要好，而且这样也能避免多个线程访问网卡时出现竞争的问题。
另外，reactor不仅负责数据的发送，还负责接收客户的请求，所以总不能让reactor阻塞等待线程池处理好数据，白白浪费cpu资源。reactor本身属于线程，而线程池还是线程
所以这就涉及到了线程之间的通信，当线程池有数据处理好的时候，应该通知reactor取走。这里采取的进程/线程通信的方法是eventfd(Linux 2.6.27及其之后版本与2.6.26
及其之前的参数有所不同)。


[200~EventLoop::wakeup() 函数的作用是通过向 evtfd 文件描述符写入数据来通知EventLoop有新的任务需要处理。这种机制实现了线程间的通信
